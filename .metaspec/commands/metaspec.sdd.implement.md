---
description: Execute toolkit implementation by processing and completing all tasks from tasks.md
---

## User Input

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

## Outline

**Goal**: Execute implementation tasks systematically, following TDD approach, respecting dependencies, and tracking progress.

**Important**: This command runs AFTER `/metaspec:tasks`. It is the execution engine for toolkit development.

---

### üìñ Navigation Guide (Quick Reference with Line Numbers)

**üéØ AI Token Optimization**: Use `read_file` with `offset` and `limit` to read only needed sections.

**Core Execution Flow** (Read sequentially):

| Step | Lines | Size | Priority | read_file Usage |
|------|-------|------|----------|-----------------|
| 1-5. Prerequisites & Setup | 86-232 | 146 lines | üî¥ MUST READ | `read_file(target_file, offset=86, limit=146)` |
| 6. Execute Implementation | 233-274 | 41 lines | üî¥ MUST READ | `read_file(target_file, offset=233, limit=41)` |
| **7. Task Execution Details** ‚≠ê | 275-525 | 251 lines | üî¥ **KEY** | See subsections below ‚¨áÔ∏è |
| 8-10. Checkpoints & Reporting | 526-617 | 91 lines | üü° Important | `read_file(target_file, offset=526, limit=91)` |
| 11-13. Validation & Saves | 618-742 | 124 lines | üü° Important | `read_file(target_file, offset=618, limit=124)` |
| 14-15. Propagation & Validation | 743-869 | 126 lines | üü° Important | `read_file(target_file, offset=743, limit=126)` |
| 16. Final Report | 870-1158 | 288 lines | üü¢ Reference | `read_file(target_file, offset=870, limit=288)` |

**üìã Section 7: Task Execution Details by Language** (251 lines):

| Component | Lines | Size | Usage |
|-----------|-------|------|-------|
| **Python Implementation** ‚≠ê | 306-508 | 203 lines | `read_file(target_file, offset=306, limit=203)` |
| **TypeScript Implementation** | 315-323 | 8 lines | `read_file(target_file, offset=315, limit=8)` |
| **Go Implementation** | 324-332 | 8 lines | `read_file(target_file, offset=324, limit=8)` |
| **Rust Implementation** | 333-341 | 8 lines | `read_file(target_file, offset=333, limit=8)` |

**Note**: Python section (203 lines) includes overview (8 lines) + `pyproject.toml` structure (97 lines) + **long command navigation guidance (74 lines)** with template and examples.

**üéØ Phase-Specific Reading** (By component type):

| Phase | Lines | Size | Usage |
|-------|-------|------|-------|
| Setup & pyproject.toml | 136-207, 306-508 | 273 lines | `read_file(target_file, offset=136, limit=71)` + `offset=306, limit=203` |
| Language-Specific Impl | 306-341 | 35 lines | `read_file(target_file, offset=306, limit=35)` |
| Command Navigation | 433-508 | 75 lines | `read_file(target_file, offset=433, limit=75)` |
| Checkpoints & Validation | 526-617 | 91 lines | `read_file(target_file, offset=526, limit=91)` |
| Progress & Error Handling | 550-617 | 67 lines | `read_file(target_file, offset=550, limit=67)` |
| Completion & Reporting | 618-742 | 124 lines | `read_file(target_file, offset=618, limit=124)` |

**üí° Typical Usage Patterns**:
```python
# Quick start: Read Prerequisites only (146 lines)
read_file(target_file, offset=86, limit=146)

# Python implementation: Read Python guidance + pyproject.toml + navigation (203 lines)
read_file(target_file, offset=306, limit=203)

# Command navigation: Read long command optimization guidance (75 lines)
read_file(target_file, offset=433, limit=75)

# Validation phase: Read validation guidance (124 lines)
read_file(target_file, offset=618, limit=124)

# Specific language: Jump to language section
read_file(target_file, offset=315, limit=8)   # TypeScript
read_file(target_file, offset=324, limit=8)   # Go
read_file(target_file, offset=333, limit=8)   # Rust
```

**Token Savings**: 
- Full file: 1158 lines (~4050 tokens)
- Prerequisites only: 146 lines (~500 tokens) ‚Üí **88% savings**
- Language-specific: 8 lines (~30 tokens) ‚Üí **99% savings** üèÜ
- Python + metadata + navigation: 203 lines (~710 tokens) ‚Üí **82% savings**
- Command navigation only: 75 lines (~260 tokens) ‚Üí **94% savings**
- Phase-specific: 67-273 lines (~230-950 tokens) ‚Üí **77-94% savings**

---

### Execution Flow

#### 1. Check prerequisites

**Required files**:
- `specs/toolkit/XXX-name/tasks.md` - Task breakdown
- `specs/toolkit/XXX-name/plan.md` - Architecture design
- `specs/toolkit/XXX-name/spec.md` - Entity definitions

**If missing**:
- Stop and instruct user to run `/metaspec:tasks` first

#### 2. Check checklist status (if exists)

**If** `specs/toolkit/XXX-name/checklists/` directory exists:

1. Scan all checklist files (*.md)
2. Count for each checklist:
   - Total items: Lines matching `- [ ]` or `- [X]` or `- [x]`
   - Completed: Lines matching `- [X]` or `- [x]`
   - Incomplete: Lines matching `- [ ]`

3. Display status table:
   ```
   | Checklist | Total | Completed | Incomplete | Status |
   |-----------|-------|-----------|------------|--------|
   | entity-design.md | 10 | 10 | 0 | ‚úì PASS |
   | validator-design.md | 8 | 5 | 3 | ‚úó FAIL |
   ```

4. **If any incomplete**:
   - Display table
   - **STOP** and ask: "Some checklists are incomplete. Proceed anyway? (yes/no)"
   - Wait for user response
   - If "no": halt
   - If "yes": continue

5. **If all complete**:
   - Display table
   - Automatically proceed

#### 3. Load implementation context

**Read all design documents**:
- `specs/toolkit/XXX-name/tasks.md` - Task list (REQUIRED)
- `specs/toolkit/XXX-name/plan.md` - Architecture (REQUIRED)
- `specs/toolkit/XXX-name/spec.md` - Entity spec (REQUIRED)
- `specs/toolkit/XXX-name/architecture.md` - Component design (if exists)
- `specs/toolkit/XXX-name/parser-design.md` - Parser interfaces (if exists)
- `specs/toolkit/XXX-name/validator-design.md` - Validation layers (if exists)
- `/specs/research.md` - Domain standards (if exists)

#### 4. Project setup verification

**Verify/create ignore files**:

1. **Check if git repository**:
   ```bash
   git rev-parse --git-dir 2>/dev/null
   ```
   - If yes: Create/verify `.gitignore`

2. **Detect technology from plan.md**:
   - Python: Create/verify `.gitignore` for Python patterns

**Python .gitignore patterns**:
```
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
venv/
.venv/
ENV/
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Testing
.pytest_cache/
.coverage
htmlcov/
.tox/
.nox/

# IDEs
.vscode/
.idea/
*.swp
*.swo
*~

# OS
.DS_Store
Thumbs.db

# Environment
.env
.env.local
*.env
```

**If .gitignore exists**:
- Verify essential patterns
- Append missing patterns

**If .gitignore missing**:
- Create with full pattern set

#### 5. Parse tasks.md structure

**Extract from tasks.md**:
- Phase list (Setup, Models, Parser, Validator, CLI, Docs, Integration)
- Task list per phase
- Task format: `- [ ] [TID] [P?] [COMP] Description with path`
- Dependencies (sequential vs parallel)

**Build execution plan**:
```
Phase 1: Setup
  - T001 [SETUP] Create directory structure
  - T002 [SETUP] Initialize pyproject.toml
  - ...

Phase 2: Models
  - T008 [MODELS] Create models.py
  - T009 [MODELS] Define Entity
  - T010 [P] [TESTS] Create test_models.py  # Can parallelize
  - T011 [P] [TESTS] Test instantiation       # Can parallelize
  - ...

[Continue for all phases]
```

#### 6. Execute implementation

**Execution rules**:

1. **Phase-by-phase**:
   - Complete Phase N before starting Phase N+1
   - Verify checkpoint after each phase

2. **TDD approach**:
   - Execute test tasks before implementation tasks
   - Example: T010 (create test file) ‚Üí T006 (implement)

3. **Respect dependencies**:
   - Sequential tasks: Execute in order
   - Parallel tasks `[P]`: Can execute simultaneously
   - Same file: Must be sequential

4. **Progress tracking**:
   - Mark completed tasks: `- [ ]` ‚Üí `- [x]`
   - Report after each task
   - Save tasks.md after each phase

**Execution order**:

```
Phase 1: Setup
  ‚Üí Execute T001, T002, T003... sequentially
  ‚Üí Checkpoint: Project structure created

Phase 2: Models
  ‚Üí Execute T008, T009 (create models.py, define entity)
  ‚Üí Execute T010-T016 in parallel (all test tasks)
  ‚Üí Checkpoint: Entity models working, tests passing

Phase 3: Parser
  ‚Üí Execute T017-T020 (implement parser)
  ‚Üí Execute T021-T024 in parallel (test tasks)
  ‚Üí Checkpoint: Parser working, tests passing

[Continue for all phases]
```

#### 7. Task execution details (ENHANCED üéØ)

**CRITICAL**: Before executing tasks, determine the implementation language and architecture from specs:

```bash
# Extract implementation details from spec.md
grep -A 30 "^## Implementation" specs/toolkit/001-*/spec.md

# Key information to extract:
# - Primary Language: Python / TypeScript / Go / Rust
# - Key Dependencies: Frameworks and libraries
# - Core Components: Which are included (Parser, Validator, CLI, etc.)
# - File Structure: From plan.md
```

**For each task**:

1. **Parse task**:
   - Extract: Task ID, Component, Description, File path
   - Check: Is parallel `[P]`?
   - Check: Is checkpoint task?

2. **Execute task BASED ON LANGUAGE** (NEW üéØ):
   
   **Determine implementation approach**:
   - Read `specs/toolkit/001-*/spec.md` ‚Üí Get primary language
   - Read `specs/domain/001-*/spec.md` ‚Üí Get entity definitions
   - Read `specs/toolkit/001-*/plan.md` ‚Üí Get architecture design
   
   **Generate code according to language**:
   
   **For Python**:
   - **SETUP tasks**: Create files/directories, initialize pyproject.toml
   - **MODELS tasks**: Generate Pydantic models from specification entities
   - **PARSER tasks**: Implement parse_spec() using PyYAML/pydantic-yaml
   - **VALIDATOR tasks**: Implement validation using Pydantic validators
   - **CLI tasks**: Create Typer commands, Rich output formatting
   - **TESTS tasks**: Write pytest tests
   - **DOCS tasks**: Write Markdown + docstrings
   
   **For TypeScript**:
   - **SETUP tasks**: Create files/directories, initialize package.json
   - **MODELS tasks**: Generate Zod schemas or interfaces from specification entities
   - **PARSER tasks**: Implement parseSpec() using js-yaml
   - **VALIDATOR tasks**: Implement validation using Zod/Yup
   - **CLI tasks**: Create Commander.js commands
   - **TESTS tasks**: Write Vitest/Jest tests
   - **DOCS tasks**: Write Markdown + TSDoc
   
   **For Go**:
   - **SETUP tasks**: Create files/directories, initialize go.mod
   - **MODELS tasks**: Generate Go structs from specification entities
   - **PARSER tasks**: Implement ParseSpec() using gopkg.in/yaml.v3
   - **VALIDATOR tasks**: Implement validation using validator package
   - **CLI tasks**: Create Cobra commands
   - **TESTS tasks**: Write Go tests
   - **DOCS tasks**: Write Markdown + godoc
   
   **For Rust**:
   - **SETUP tasks**: Create files/directories, initialize Cargo.toml
   - **MODELS tasks**: Generate Rust structs with Serde from specification entities
   - **PARSER tasks**: Implement parse_spec() using serde_yaml
   - **VALIDATOR tasks**: Implement validation using validator crate
   - **CLI tasks**: Create Clap commands
   - **TESTS tasks**: Write Rust tests
   - **DOCS tasks**: Write Markdown + rustdoc

   **Code Generation Strategy**:
   - Read domain/001-*/spec.md for entity definitions
   - Generate models/types/structs that match specification entities
   - Implement parser that creates instances of these models
   - Implement validator that enforces specification rules
   - Implement CLI commands defined in toolkit spec
   - Follow architecture from plan.md

   **Python pyproject.toml Structure** (T002 - Initialize/Update pyproject.toml):
   
   **CRITICAL**: Update `[tool.metaspec]` section based on actual implemented capabilities.
   
   ```toml
   [project]
   name = "your-speckit"
   version = "0.1.0"
   # ... (standard fields)
   
   [tool.metaspec]
   generated_by = "0.9.5"
   domain = "your-domain"
   lifecycle = "greenfield"  # Optional
   
   # CLI commands this speckit provides
   cli_commands = ["init", "validate", "generate", "info"]
   
   # Slash command system type (see calculation rules below)
   sd_type = "custom"
   
   # Slash commands this speckit provides (for AI agents)
   slash_commands = ["show-spec", "get-template"]
   ```
   
   **Update Procedure**:
   
   1. **CLI Commands**: List all commands defined in `src/{package}/cli.py`
      - Extract from `@app.command()` decorators
      - Or list from `[project.scripts]` entrypoints
      - Update: `cli_commands = ["cmd1", "cmd2", ...]`
   
   2. **Slash Commands**: List all `.md` files in `.metaspec/commands/`
      - Each `.md` file = one slash command
      - Extract name from filename (e.g., `show-spec.md` ‚Üí `show-spec`)
      - Update: `slash_commands = ["cmd1", "cmd2", ...]`
   
   3. **sd_type**: Classify the command system
      - **SD-X systems** (following `sd<domain>` pattern):
        * `"sdd"` - Spec-Driven Development (from library/sdd/)
        * `"sds"` - Spec-Driven Specification (from library/sds/)
        * `"sdm"` - Spec-Driven Marketing (custom marketing workflow)
        * `"sdt"`, `"sdo"`, etc. - Other SD-X extensions
      - **Non-SD-X**:
        * `"custom"` - Custom commands not following SD-X pattern
        * `"none"` - No slash commands
      - **Mixed**: `["custom", "sdd"]` or `["sdm", "sdd"]` (array)
   
   **Example Scenarios**:
   
   **Scenario 1: Custom Commands** (MCP - not SD-X)
   ```toml
   cli_commands = ["init", "validate", "generate"]
   sd_type = "custom"
   slash_commands = ["show-spec", "get-template"]
   ```
   
   **Scenario 2: SDD System** (Spec-Kit Pattern)
   ```toml
   cli_commands = ["init", "validate"]
   sd_type = "sdd"
   slash_commands = ["specify", "plan", "implement"]
   ```
   
   **Scenario 3: SDM System** (Spec-Driven Marketing)
   ```toml
   cli_commands = ["info", "init", "validate"]
   sd_type = "sdm"
   slash_commands = ["marketspec.constitution", "marketspec.specify", "marketspec.plan", ...]
   ```
   
   **Scenario 4: Mixed Systems**
   ```toml
   cli_commands = ["init", "validate", "analyze"]
   sd_type = ["custom", "sdd"]
   slash_commands = ["discover", "specify", "plan", "generate"]
   ```
   
   **Purpose**: This metadata enables community to:
   - Discover speckit capabilities (what CLI commands available?)
   - Understand AI command system type (what workflow pattern?)
   - Search/filter speckits by capabilities

3. **Optimize Long Slash Commands** ‚≠ê NEW

   **When**: Slash command files in `.metaspec/commands/` or `templates/*/commands/` exceed 500 lines
   
   **Why**: Save 70-90% token consumption when AI reads commands
   
   **How**: Add precision-guided navigation at file start (after frontmatter)
   
   **Navigation Template**:
   
   ```markdown
   ---
   name: command-name
   description: Command description
   ---
   
   # /command-name
   
   üìñ **Navigation Guide** (Token Optimization)
   
   **File Size**: {line_count} lines (~{token_estimate} tokens)
   **Recommended**: Read specific sections to save 70-90% tokens
   
   | Section | Lines | Size | Usage |
   |---------|-------|------|-------|
   | 1. Purpose & Usage | 1-100 | 100 lines | `read_file(target, offset=1, limit=100)` |
   | 2. Input Analysis | 101-250 | 150 lines | `read_file(target, offset=101, limit=150)` |
   | 3. Processing Rules | 251-500 | 250 lines | `read_file(target, offset=251, limit=250)` |
   | 4. Output Format | 501-750 | 250 lines | `read_file(target, offset=501, limit=250)` |
   | 5. Examples | 751-{end} | {size} lines | `read_file(target, offset=751, limit={size})` |
   
   **üí° Typical Usage**:
   ```python
   # Quick reference: Read purpose only (100 lines)
   read_file(target, offset=1, limit=100)
   
   # Specific section: Read processing rules (250 lines)
   read_file(target, offset=251, limit=250)
   
   # Full understanding: Read all sections sequentially
   ```
   
   **Token Savings**:
   - Full file: {line_count} lines (~{token_estimate} tokens)
   - Single section: 100-250 lines (~350-850 tokens) ‚Üí **70-90% savings** üèÜ
   
   ---
   
   [Actual command content starts here...]
   ```
   
   **Guidelines**:
   - **Threshold**: Add navigation for commands >500 lines
   - **Sections**: 4-6 logical sections, each 100-300 lines
   - **Line Numbers**: Calculate actual boundaries (use `wc -l`)
   - **Token Estimate**: ~3.5 tokens per line average
   - **Savings**: Emphasize 70-90% for section reads
   
   **Section Design**:
   - Section 1: Purpose, usage, quick reference
   - Section 2-N: Major logical blocks (input, rules, output, etc.)
   - Last section: Examples and edge cases
   
   **Example Calculation**:
   ```bash
   # For 847-line command
   Section 1: Purpose & Usage (1-100, 100 lines)
   Section 2: Input Analysis (101-250, 150 lines)
   Section 3: Processing Rules (251-500, 250 lines)
   Section 4: Output Format (501-700, 200 lines)
   Section 5: Examples (701-847, 147 lines)
   
   Token savings: 847 lines ‚Üí 150 lines = 82% savings
   ```

4. **Verify task**:
   - **SETUP**: Files created, configs valid, dependencies installable
   - **MODELS**: Models instantiable, match specification entities
   - **PARSER**: Can parse valid/invalid specs, error handling works
   - **VALIDATOR**: Enforces all specification rules, error messages clear
   - **CLI**: Commands execute, help text clear, output formatted
   - **TESTS**: Tests pass, coverage adequate
   - **DOCS**: Documentation complete, examples work

5. **Mark complete**:
   - Update tasks.md: `- [ ] T001` ‚Üí `- [x] T001`
   - Report: "‚úÖ T001 complete: Created directory structure"

6. **Handle errors**:
   - If task fails: Report error, suggest fix
   - If sequential task fails: Halt phase
   - If parallel task fails: Continue others, report at end

#### 8. Phase checkpoints

**After each phase**:

1. **Verify checkpoint criteria**:
   - Phase 1: `‚úÖ Project structure created, tests can run`
   - Phase 2: `‚úÖ Entity models working, all tests passing`
   - Phase 3: `‚úÖ Parser working, handles all error cases`
   - Phase 4: `‚úÖ All validation layers working, error messages clear`
   - Phase 5: `‚úÖ CLI working, init and validate commands functional`
   - Phase 6: `‚úÖ Documentation complete, examples working`
   - Phase 7: `‚úÖ All integration tests passing, constitution compliant`

2. **If checkpoint fails**:
   - Stop phase
   - Report failing criteria
   - Suggest fixes
   - Ask user: "Fix issues and continue? (yes/no)"

3. **If checkpoint passes**:
   - Report success
   - Save tasks.md
   - Proceed to next phase

#### 9. Progress reporting

**After each task**:
```
‚úÖ T001 complete: Created directory structure
   Files created:
   - src/api_test_kit/
   - tests/unit/
   - tests/integration/
   - templates/
   - memory/
```

**After each phase**:
```
‚úÖ Phase 1 complete: Project Setup
   Tasks: 7/7 complete
   Time: 5 minutes
   
   Checkpoint: ‚úÖ Project structure created, tests can run
   
   Next: Phase 2 (Entity Models)
```

**Overall progress**:
```
üìä Implementation Progress: 14/59 tasks (24%)
   Phase 1: ‚úÖ 7/7
   Phase 2: üîÑ 7/9 (in progress)
   Phase 3: ‚è≥ 0/8
   Phase 4: ‚è≥ 0/13
   Phase 5: ‚è≥ 0/11
   Phase 6: ‚è≥ 0/5
   Phase 7: ‚è≥ 0/6
```

#### 10. Error handling

**Error types**:

1. **File creation error**:
   - Report: Cannot create file [path]
   - Suggest: Check permissions, disk space
   - Action: Fix and retry task

2. **Test failure**:
   - Report: Test [name] failed
   - Show: Test output, error message
   - Suggest: Fix implementation, rerun tests
   - Action: Fix and retry task

3. **Import error**:
   - Report: Cannot import [module]
   - Suggest: Check dependencies, install packages
   - Action: Fix dependencies, retry task

4. **Validation error**:
   - Report: Code doesn't match spec
   - Show: Spec requirement, actual implementation
   - Suggest: Adjust code to match spec
   - Action: Fix and retry task

5. **Constitution violation**:
   - Report: Implementation violates [principle]
   - Show: Principle text, violation
   - Suggest: Refactor to align with principle
   - Action: Ask user to approve deviation or fix

#### 11. Completion validation

**After all phases**:

1. **Verify completeness**:
   - [ ] All tasks marked `[x]`
   - [ ] All checkpoints passed
   - [ ] All tests passing
   - [ ] No linter errors
   - [ ] Constitution compliant

2. **Run final checks**:
   ```bash
   # Run tests
   pytest tests/ --cov

   # Run linter
   mypy src/ --strict
   ruff check src/

   # Test CLI
   [toolkit-name] --help
   [toolkit-name] init test.yaml
   [toolkit-name] validate test.yaml
   ```

3. **Constitution compliance**:
   - ‚úÖ Entity-First: Model has 3-5 core fields
   - ‚úÖ Validator Extensibility: register_validator() works
   - ‚úÖ Spec-First: Users write specs first
   - ‚úÖ AI-Agent Friendly: Error messages actionable
   - ‚úÖ Progressive Enhancement: MVP feature set
   - ‚úÖ Domain Specificity: Domain rules implemented

4. **Generate report**:
   ```
   ‚úÖ Implementation complete
   
   üìä Summary:
   - Total tasks: 59/59 complete
   - Time: 3 days
   - Test coverage: 92%
   - Constitution: COMPLIANT
   
   üì¶ Deliverables:
   - ‚úÖ Entity models (models.py)
   - ‚úÖ Parser (parser.py)
   - ‚úÖ Validator (validator.py)
   - ‚úÖ CLI (cli.py)
   - ‚úÖ Tests (90% coverage)
   - ‚úÖ Documentation (README, AGENTS.md)
   
   üéØ MVP Status:
   - v0.1.0 ready for release
   - Features: init, validate commands
   - Quality: All tests passing
   
   üîÑ Next steps:
   1. Run /metaspec:checklist to verify quality
   2. Test with real-world specs
   3. Update CHANGELOG.md
   4. Create git tag v0.1.0
   5. Publish to PyPI
   
   üí° Suggested commit message:
      feat: implement [toolkit-name] v0.1.0 MVP
   ```

#### 12. Incremental saves

**Save progress frequently**:
- Save tasks.md after each phase
- Commit code after each checkpoint
- Push to git after significant milestones

**Suggested commit messages**:
```bash
git commit -m "feat: complete Phase 1 (Project Setup)"
git commit -m "feat: implement entity models (Phase 2)"
git commit -m "feat: implement parser (Phase 3)"
git commit -m "feat: implement validator (Phase 4)"
git commit -m "feat: implement CLI commands (Phase 5)"
git commit -m "docs: add documentation (Phase 6)"
git commit -m "test: add integration tests (Phase 7)"
```

#### 13. Generate Implementation Report

**Purpose**: Create detailed record of toolkit implementation for quality assurance.

After all tasks complete, create `specs/toolkit/{toolkit_id}/IMPLEMENTATION.md` with HTML comment:

```html
<!--
Toolkit Implementation Report
==============================
Toolkit: {toolkit_id} | Language: {Python | TS | Go | Rust} | Architecture: {Monolithic | Modular | Plugin}
Duration: {start_date} - {end_date} ({days}d)

Summary:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Tasks: {completed}/{total} ({percent}%) | By Phase: Setup: {p1}/{p1t}, Models: {p2}/{p2t}, Parser: {p3}/{p3t}, Validator: {p4}/{p4t}, CLI: {p5}/{p5t}, Tests: {p6}/{p6t}, Docs: {p7}/{p7t}

Files:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Created: {created} files ({loc} LOC) | Updated: tasks.md, README.md, tests ({test_count} files)

Quality:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Test Coverage: {coverage}% | Tests: {unit_tests} unit, {integration_tests} integration, {‚úÖ All pass | ‚ö†Ô∏è {failed} fail}
Linter: {‚úÖ Clean | ‚ö†Ô∏è {count}W | ‚ùå {count}E} | Type Check: {‚úÖ Pass | ‚ö†Ô∏è {count} issues}
Domain Spec: {implemented}/{total} entities, {implemented}/{total} operations, {implemented}/{total} rules
Completeness: {percent}% | Dependencies: {count} external

Constitution: {‚úÖ Compliant | ‚ö†Ô∏è Check: {issues}}

Next: Run analyze ‚Üí Fix issues ‚Üí Publish ‚Üí Update docs

Generated by: /metaspec.sdd.implement
-->
```

**Also update tasks.md**:
Mark all completed tasks with `[x]`.

#### 14. Consistency Propagation and Impact Analysis

**Purpose**: Verify implementation aligns with specifications and quality standards.

#### A. Domain Specification Compliance Check (CRITICAL)

```bash
# Compare implemented code with domain specification
cat specs/domain/{domain_spec_id}/spec.md

# Check entity implementation
grep "class.*Entity" src/{package}/*.py  # or .ts/.go
```

**Verify**:
- [ ] All domain entities have corresponding model classes
- [ ] All entity fields implemented correctly
- [ ] All domain operations have implementation
- [ ] All validation rules enforced in code

**Action if missing**:
- ‚ö†Ô∏è **CRITICAL**: Domain spec not fully implemented
- Add missing implementations

#### B. Toolkit Specification Alignment Check

```bash
# Verify components match toolkit spec
cat specs/toolkit/{toolkit_id}/spec.md
```

**Verify**:
- [ ] All specified components implemented
- [ ] Language matches spec (Python/TypeScript/Go/Rust)
- [ ] Architecture matches spec (Monolithic/Modular/Plugin)
- [ ] CLI commands match spec

#### C. Code Quality Checks

```bash
# Run linter
{IF Python}:
ruff check src/{package}/
mypy src/{package}/
{ELSE IF TypeScript}:
npm run lint
npm run type-check
{ELSE IF Go}:
go vet ./...
golangci-lint run
{ELSE IF Rust}:
cargo clippy
{END IF}

# Run tests
{IF Python}:
pytest tests/ -v --cov=src/{package}
{ELSE IF TypeScript}:
npm test
{ELSE IF Go}:
go test ./... -cover
{ELSE IF Rust}:
cargo test
{END IF}
```

**Quality gates**:
- [ ] No linter errors
- [ ] Type checking passes
- [ ] All tests passing
- [ ] Test coverage > 80%

#### D. Tasks Status Synchronization

```bash
# Mark all tasks complete in tasks.md
cat specs/toolkit/{toolkit_id}/tasks.md
```

**Update**:
- Mark all tasks `[x]`
- Add completion notes

#### E. Related Files Impact Check

```bash
# Check if domain spec or documentation needs updates
find specs/domain/ docs/ -type f -name "*.md"
```

**If toolkit changes affect**:
- Domain specification ‚Üí Note for review
- Project documentation ‚Üí Update README/AGENTS.md

#### 15. Completion Validation (Enhanced)

**Final quality checks**:

```bash
# Comprehensive validation
/metaspec.sdd.analyze
```

**Manual checks**:
- [ ] All CLI commands work correctly
- [ ] Parser handles all input formats
- [ ] Validator enforces all rules from domain spec
- [ ] Error messages are clear and helpful
- [ ] Examples in README work
- [ ] AGENTS.md has complete usage guide
- [ ] Package can be installed successfully

**Code quality final check**:
- [ ] No TODO/FIXME comments left
- [ ] All functions have docstrings
- [ ] All complex logic has comments
- [ ] No debug print statements
- [ ] No hardcoded values (use config)

**Constitution alignment final check**:
- [ ] Models defined before business logic (Entity-First)
- [ ] Validator is extensible (plugins/rules can be added)
- [ ] Implementation strictly follows domain spec (Spec-First)
- [ ] CLI is intuitive for AI agents (AI-Agent Friendly)
- [ ] MVP functional, advanced features optional (Progressive)
- [ ] Comprehensive test coverage (Automated Quality)

#### 16. Report Completion (Enhanced)

Provide comprehensive implementation summary:

```
‚úÖ Toolkit implementation complete

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

üìä Implementation Summary:
   Toolkit: {toolkit_id}
   Language: {Python | TypeScript | Go | Rust}
   Architecture: {Monolithic | Modular | Plugin-based}
   Duration: {start_date} to {end_date} ({days} days)

   Tasks Completed: {completed}/{total} ({percentage}%)
   - Setup & Infrastructure: {phase1_tasks} tasks ‚úÖ
   - Entity Models: {phase2_tasks} tasks ‚úÖ
   - Parser: {phase3_tasks} tasks ‚úÖ
   - Validator: {phase4_tasks} tasks ‚úÖ
   - CLI Commands: {phase5_tasks} tasks ‚úÖ
   - Tests: {phase6_tasks} tasks ‚úÖ
   - Documentation: {phase7_tasks} tasks ‚úÖ

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

üìÅ Files Created/Updated:

   **Created** ({count} files, {loc} lines):
   ‚úÖ Models: src/{package}/models.{ext}
      - {entity_count} entity classes
      - {lines} lines
   
   ‚úÖ Parser: src/{package}/parser.{ext}
      - Parses {format} files
      - {lines} lines
   
   ‚úÖ Validator: src/{package}/validator.{ext}
      - Enforces {rule_count} validation rules
      - {lines} lines
   
   ‚úÖ CLI: src/{package}/cli.{ext}
      - {command_count} commands (init, validate, generate, etc.)
      - {lines} lines
   
   ‚úÖ Tests: tests/ ({test_count} test files)
      - {unit_test_count} unit tests
      - {integration_test_count} integration tests
      - Coverage: {coverage}% ‚úÖ

   **Updated**:
   ‚úÖ tasks.md: All tasks marked complete
   ‚úÖ README.md: Usage guide and examples
   ‚úÖ AGENTS.md: AI agent usage patterns
   ‚úÖ IMPLEMENTATION.md: Full implementation report

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‚úÖ Validation Results:

   **Code Quality**: {‚úÖ Excellent | ‚ö†Ô∏è Good | ‚ùå Needs Work}
   - Total LOC: {loc} lines
   - Source: {source_loc} lines
   - Tests: {test_loc} lines
   - Test coverage: {coverage}% ‚úÖ
   - Linter: {‚úÖ No issues | ‚ö†Ô∏è {count} warnings}
   - Type checking: {‚úÖ Passed | ‚ö†Ô∏è {count} issues}

   **Domain Specification Compliance**: {‚úÖ Complete | ‚ö†Ô∏è Partial}
   - Domain Spec: specs/domain/{domain_spec_id}/spec.md
   - Entities: {implemented}/{total} implemented ({percentage}%)
   - Operations: {implemented}/{total} implemented ({percentage}%)
   - Validation rules: {implemented}/{total} enforced ({percentage}%)
   {IF incomplete}:
   ‚ö†Ô∏è  Not fully implemented:
   - Missing: {list}
   - Action: Add in next iteration
   {END IF}

   **Toolkit Specification Alignment**: {‚úÖ Aligned | ‚ö†Ô∏è Deviation}
   - Components: All specified components implemented ‚úÖ
   - Language: {language} matches spec ‚úÖ
   - Architecture: {type} matches spec ‚úÖ
   - CLI commands: {implemented}/{specified} ‚úÖ

   **Tests**: {‚úÖ All Passing | ‚ö†Ô∏è {failed} Failing | ‚ùå Not Run}
   - Unit tests: {unit_passed}/{unit_total} passing
   - Integration tests: {int_passed}/{int_total} passing
   - Coverage: {coverage}% (target: >80%)
   {IF failing_tests}:
   ‚ö†Ô∏è  Failing tests:
   - {test_1}: {reason}
   - {test_2}: {reason}
   Action: Fix before release
   {END IF}

   **Constitution Compliance**: {‚úÖ Fully Compliant | ‚ö†Ô∏è Minor Issues}
   - Entity-First Design: {‚úÖ | ‚ö†Ô∏è} {comment}
   - Validator Extensibility: {‚úÖ | ‚ö†Ô∏è} {comment}
   - Spec-First Development: {‚úÖ | ‚ö†Ô∏è} {comment}
   - AI-Agent Friendly: {‚úÖ | ‚ö†Ô∏è} {comment}
   - Progressive Enhancement: {‚úÖ | ‚ö†Ô∏è} {comment}
   - Automated Quality: {‚úÖ | ‚ö†Ô∏è} {comment}

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

üéØ Impact Analysis:

   **Dependencies**:
   üì¶ External Packages ({count}):
   {FOR each dependency}:
   - {package}: {version}
   {END FOR}

   **Related Files to Review**:
   {IF domain_spec_impact}:
   üìÑ Domain Specification:
   - specs/domain/{domain_spec_id}/spec.md
   - Review if entity definitions need updates
   {END IF}

   {IF doc_impact}:
   üìö Documentation:
   - README.md: Updated with usage examples ‚úÖ
   - AGENTS.md: Added AI agent patterns ‚úÖ
   - docs/: {updated_count} files updated
   {END IF}

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

üìà Metrics:

   **Code Statistics**:
   - Total lines: {total_loc}
   - Source code: {source_loc} lines ({percentage}%)
   - Test code: {test_loc} lines ({percentage}%)
   - Documentation: {doc_loc} lines ({percentage}%)
   - Test-to-code ratio: 1:{ratio}

   **Complexity**:
   - Files: {file_count}
   - Classes: {class_count}
   - Functions: {function_count}
   - Average function length: {avg_lines} lines

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

üîÑ Next Steps:

   1. Run final analysis:
      ‚Üí /metaspec.sdd.analyze
      
   2. Generate quality checklist:
      ‚Üí /metaspec.sdd.checklist
      
   3. Test installation:
      ‚Üí pip install -e .  # or npm install / go build
      ‚Üí {package-name} --help
      
   4. Verify CLI commands:
      ‚Üí {package-name} init
      ‚Üí {package-name} validate example.yaml
      ‚Üí {package-name} generate

‚ö†Ô∏è  Follow-up TODOs:

   {IF linter_issues}:
   - [ ] Fix linter warnings: {count} issues
   {END IF}
   {IF type_issues}:
   - [ ] Fix type-check issues: {count} issues
   {END IF}
   {IF failing_tests}:
   - [ ] Fix failing tests: {list}
   {END IF}
   {IF coverage_low}:
   - [ ] Improve test coverage: {current}% ‚Üí 80%+
   {END IF}
   {IF incomplete_impl}:
   - [ ] Complete domain spec implementation: {missing_items}
   {END IF}
   - [ ] Run final quality check: /metaspec.sdd.analyze
   - [ ] Publish package (if ready)

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

üí° Suggested Commit Message:

   feat(toolkit): complete {toolkit_id} implementation
   
   Implementation:
   - Language: {language}
   - Architecture: {type}
   - Components: Parser, Validator, CLI, {others}
   - LOC: {source_loc} source, {test_loc} tests
   
   Quality:
   - Tests: {test_count} tests, {coverage}% coverage
   - Domain spec: {percentage}% implemented
   - Linter: {‚úÖ Clean | ‚ö†Ô∏è {count} warnings}
   
   {IF issues}:
   Known Issues:
   - {issue_1}
   - {issue_2}
   {END IF}
```

## Important Notes

1. **TDD is mandatory**
   - Write tests before implementation
   - Run tests after each implementation
   - Maintain > 90% coverage

2. **Phase isolation**
   - Don't start Phase N+1 until Phase N complete
   - Verify checkpoint after each phase
   - Fix issues before proceeding

3. **Parallel execution**
   - Tasks marked `[P]` can run in parallel
   - Only if they touch different files
   - Report all results together

4. **Constitution alignment**
   - Check principles after each phase
   - Halt if violation detected
   - Get user approval for deviations

5. **Error recovery**
   - Clear error messages
   - Actionable suggestions
   - Allow user to fix and retry

6. **Progress visibility**
   - Report after each task
   - Show percentage complete
   - Estimate time remaining

## Example: Phase 2 Execution

**Phase 2: Entity Models**

```
Starting Phase 2: Entity Models (9 tasks)

‚úÖ T008: Create src/api_test_kit/models.py
   File created: src/api_test_kit/models.py

‚úÖ T009: Define APITest model
   Fields added:
   - name: str (required)
   - endpoint: str (required)
   - method: str (required)
   - headers: Optional[Dict]
   - body: Optional[Any]
   - assertions: Optional[List]

‚úÖ T010: Add field validators
   Validators added:
   - method: Must be GET/POST/PUT/DELETE
   - endpoint: Must start with /

‚úÖ T011: Add model methods
   Methods added:
   - model_dump()
   - model_validate()
   - model_json_schema()

‚úÖ T012: Add docstrings
   Docstrings added for: APITest, all fields

üß™ Running test tasks in parallel...

‚úÖ T013: Create tests/unit/test_models.py
‚úÖ T014: Test APITest instantiation (3 test cases)
‚úÖ T015: Test field validation (4 test cases)
‚úÖ T016: Test model methods (3 test cases)

All tests passing: 10/10 ‚úÖ

‚úÖ Phase 2 checkpoint: Entity models working, all tests passing

Phase 2 complete: 9/9 tasks (15 minutes)

Next: Phase 3 (Parser)
```

